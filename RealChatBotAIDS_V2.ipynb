{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4BFYDOFfQHt",
        "outputId": "a438eeb2-56dc-4c47-e812-f2dd5bfa8298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Assistant: My name is AI Assistant. I will answer your queries about AI and Data Science. If you want to exit, type 'bye'.\n",
            "NLP?\n",
            "AI Assistant: Natural language processing is a field of artificial intelligence that focuses on enabling computers to understand, interpret, and generate human language, including tasks such as text classification, sentiment analysis, machine translation, and question answering.\n",
            "hey\n",
            "AI Assistant: Hi! How can I help you with AI or Data Science?\n",
            "bye\n",
            "AI Assistant: Bye! Take care.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Télécharger les ressources NLTK\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "# Lemmatiseur pour normaliser les mots\n",
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def LemNormalize(text):\n",
        "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "\n",
        "# Charger le fichier CSV avec pandas\n",
        "df = pd.read_csv('/content/DataScience QA.csv', encoding='utf-8', on_bad_lines='skip')\n",
        "\n",
        "# Assumant que le fichier CSV a deux colonnes : 'question' et 'answer'\n",
        "questions = df['Question'].tolist()\n",
        "answers = df['Answer'].tolist()\n",
        "\n",
        "# Fonction pour les salutations\n",
        "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\"]\n",
        "GREETING_RESPONSES = [\n",
        "    \"Hi! How can I help you with AI or Data Science?\",\n",
        "    \"Hello! Do you have any questions about AI or Data Science?\",\n",
        "    \"Hey! Feel free to ask me anything related to AI or Data Science.\"\n",
        "]\n",
        "\n",
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)\n",
        "\n",
        "# Initialisation du TF-IDF Vectorizer\n",
        "TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "tfidf = TfidfVec.fit_transform(questions)\n",
        "\n",
        "# Fonction pour la réponse\n",
        "def response(user_response):\n",
        "    robo_response = ''\n",
        "    tfidf_user_response = TfidfVec.transform([user_response])\n",
        "    vals = cosine_similarity(tfidf_user_response, tfidf)\n",
        "    idx = vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "\n",
        "    # Si la similarité est trop faible, on demande une clarification\n",
        "    if flat[-2] == 0:\n",
        "        robo_response = \"I'm sorry, I don't understand. Could you please rephrase?\"\n",
        "    else:\n",
        "        # Retourner seulement la réponse correspondant à la question\n",
        "        robo_response = answers[idx]\n",
        "\n",
        "    return robo_response\n",
        "\n",
        "# Fonction principale du chatbot\n",
        "def chatbot():\n",
        "    print(\"AI Assistant: My name is AI Assistant. I will answer your queries about AI and Data Science. If you want to exit, type 'bye'.\")\n",
        "    flag = True\n",
        "    while flag:\n",
        "        user_response = input().lower()\n",
        "        if user_response != 'bye':\n",
        "            if greeting(user_response) is not None:\n",
        "                print(\"AI Assistant: \" + greeting(user_response))\n",
        "            else:\n",
        "                print(\"AI Assistant: \" + response(user_response))\n",
        "        else:\n",
        "            flag = False\n",
        "            print(\"AI Assistant: Bye! Take care.\")\n",
        "\n",
        "# Lancer le chatbot\n",
        "chatbot()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save answers after the chat\n",
        "with open('answers.pkl', 'wb') as f:\n",
        "    pickle.dump(answers, f)\n",
        "\n",
        "print(\"Model components saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5zixL7NliVS",
        "outputId": "31974699-abd6-4a2a-fc06-01eb741685fc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model components saved successfully!\n"
          ]
        }
      ]
    }
  ]
}